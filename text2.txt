Die vorliegende Arbeit befasst sich mit der Vorhersage von High-Level-Metriken, wie den UX-Performance-Scores des Baymard Institute oder den Metriken von Google PageSpeed Insights, auf Basis von Low-Level-Metriken wie der Anzahl spezifischer HTML-Elemente. Die gewonnenen Erkenntnisse liefern wertvolle Einblicke in mögliche Zusammenhänge zwischen strukturellen Merkmalen einer Webseite und deren Nutzerfreundlichkeit sowie Ladegeschwindigkeit. Dennoch bleiben zahlreiche offene Fragen bestehen, die weiterführende Forschung notwendig machen.

Ein zentraler Ansatz für zukünftige Forschungen könnte die Berücksichtigung weiterer, leicht zu erhebender Metriken sein. Während sich diese Arbeit auf die Anzahl bestimmter HTML-Tags konzentrierte, wäre es denkbar, die Analyse auf weitere HTML-Elemente auszudehnen oder zusätzliche Faktoren wie die Anzahl und Größe der eingebundenen JavaScript- und CSS-Dateien zu untersuchen. Auch die Tiefe des DOM-Baums könnte eine entscheidende Rolle für die User Experience spielen, da tief verschachtelte Strukturen potenziell zu längeren Rendering-Zeiten führen. Eine umfassendere Analyse der Netzwerkaktivität einer Webseite könnte zudem aufzeigen, inwiefern die Anzahl der geladenen externen Ressourcen Einfluss auf die UX-Performance nimmt. Darüber hinaus könnte der Untersuchungsbereich erheblich erweitert werden, indem nicht nur die Webseiten analysiert werden, die vom Baymard Institute bewertet wurden, sondern auch weitere Webseiten des E-Commerce-Sektors sowie andere Branchen. Eine branchenübergreifende Untersuchung könnte neue Muster offenlegen und die Übertragbarkeit der Ergebnisse validieren. Zusätzlich könnte es sinnvoll sein, neben dem Baymard Institute weitere Institutionen und Analysewerkzeuge in die Bewertung einzubeziehen, um bestehende Erkenntnisse zu bestätigen oder alternative Bewertungsmethoden zu erproben. Allerdings wäre eine solch umfassende Erweiterung der Analyse mit erheblichen Kosten verbunden, insbesondere wenn eine große Anzahl von Webseiten durch UX-Experten manuell bewertet werden müsste.

Neben diesen theoretischen Erweiterungsmöglichkeiten ergeben sich aus der Arbeit auch praktische Implikationen. Der Ansatz, mit Hilfe von Low-Level-Metriken Vorhersagen über High-Level-Metriken zu treffen, könnte als effiziente und kostengünstige Methode dienen, um aufwendige Performance- und UX-Analysen zu ergänzen. Wenn es gelingt, eine robuste Korrelation zwischen bestimmten Low-Level-Metriken und UX-Performance-Scores herzustellen, könnte dies eine Möglichkeit bieten, Webseiten mit schlechter User Experience schnell zu identifizieren und gezielt zu optimieren. Dies würde insbesondere Unternehmen und Webentwicklern ermöglichen, ohne tiefgehende manuelle Analysen erste Indikatoren für Optimierungsbedarf zu erhalten. Allerdings bleibt zu beachten, dass solche Vorhersagen stets mit einer gewissen Unsicherheit behaftet sind und detaillierte Analysen nicht vollständig ersetzen können. Sollten sich jedoch verlässliche Low-Level-Metriken identifizieren lassen, die eine hinreichend präzise Vorhersage der UX-Qualität ermöglichen, könnte dies insbesondere für große Plattformen mit zahlreichen Webseiten von erheblichem Nutzen sein. So ließen sich in groß angelegten Analysen problematische Webseiten frühzeitig erkennen und gezielt verbessern.

Obwohl die Ergebnisse dieser Arbeit wertvolle Erkenntnisse liefern, sind sie mit einigen Einschränkungen verbunden. Eine der wesentlichen Herausforderungen liegt in der Verteilung der Datenpunkte. Ein Großteil der analysierten Webseiten weist eine konzentration der Datenpunkte im unteren Bereich der Skala auf. Dazu zählen vorrangig die Anzahl der verschiedenen HTML-Tags und die PSI-Metriken. Beide Metriken zeigen vermehrt Datenpunkte mit geringen Werten. Dies könnte dazu führen, dass die verwendeten Korrelationsmethoden, die auf gleichmäßig verteilte Daten ausgelegt sind, weniger präzise Ergebnisse liefern. Zur Minderung dieses Effekts wurde in dieser Arbeit teilweise nur der mittlere Bereich der Datenmenge betrachtet, um Verzerrungen durch extreme Ausreißer zu minimieren. Dennoch stellt dies keine perfekte Lösung dar. Zusätzlich sind alternative Methoden zur Analyse nicht-linearer Zusammenhänge in zukünftigen Arbeiten denkbar, um weitere Korrelationsmöglichkeiten prüfen zu können. Ein weiterer kritischer Aspekt betrifft die UX-Performance-Scores des Baymard Institute. Zwar werden die Webseiten regelmäßig analysiert, jedoch ist nicht transparent, welche Version einer Webseite tatsächlich getestet wurde. Dies führt zu Unsicherheiten hinsichtlich der Vergleichbarkeit der Ergebnisse, da die in dieser Arbeit analysierten Versionen möglicherweise von denen abweichen, die das Baymard Institute bewertet hat. Zudem sind einige der untersuchten Webseiten aus bestimmten geografischen Regionen nicht zugänglich, was potenziell zu fehlerhaften Analysen führen kann. Falls eine Webseite für den deutschen Markt nicht verfügbar ist, kann dies dazu führen, dass entweder eine Platzhalterseite des Unternehmens oder eine Webseite von einem anderen Unternehmen mit ähnlichem Namen analysiert wurde. Diese Einschränkungen erschweren die automatisierte Datenauswertung und machen es nachträglich nahezu unmöglich, Fehler vollständig zu identifizieren und zu korrigieren.

Ein weiterer Aspekt, welcher die Aussagekraft der Ergebnisse begrenzt, ist die fortlaufende technologische Entwicklung. Diese Arbeit stellt lediglich eine Momentaufnahme dar, da sich Webseiten kontinuierlich weiterentwickeln und regelmäßig Änderungen an ihrer Struktur vornehmen. Mit jeder neuen Version einer Webseite ändern sich möglicherweise die Anzahl der HTML-Tags, die DOM-Struktur und die eingebundenen Ressourcen, was direkte Auswirkungen auf die UX-Performance-Scores des Baymard Institute und die PSI-Metriken hat. Somit sind die in dieser Arbeit erhobenen Daten nur als aktueller Status zu verstehen, der in zukünftigen Analysen überprüft und aktualisiert werden müsste. Technologische Neuerungen haben das Potenzial, die Ladezeiten erheblich zu verbessern oder das Webdesign grundlegend zu verändern. Betrachtet man beispielsweise Webseiten vor zehn Jahren, so wird deutlich, dass sowohl das Design als auch die Download- und Uploadgeschwindigkeiten signifikante Entwicklungen durchlaufen haben. Solche drastischen Veränderungen verdeutlichen, dass die in dieser Arbeit gewonnenen Erkenntnisse möglicherweise nur für eine begrenzte Zeit Gültigkeit besitzen. Die schnelle Entwicklung des Internets führt dazu, dass heutige Best Practices und Analyseergebnisse in wenigen Jahren bereits überholt sein könnten. Dies bedeutet auch, dass Methoden zur Bewertung von Webseiten kontinuierlich weiterentwickelt und an neue Standards angepasst werden müssen. Nur so können valide und zuverlässige Aussagen über die User Experience und Performance einer Webseite getroffen werden.

Die Forschung zur Optimierung der User Experience im Web steht damit vor der Herausforderung, mit den sich ständig verändernden technischen Rahmenbedingungen Schritt zu halten. In zukünftigen Arbeiten könnte nicht nur eine breitere und diversifizierte Datengrundlage genutzt werden, sondern auch maschinelles Lernen und KI-gestützte Analyseverfahren eine größere Rolle spielen, um Zusammenhänge zwischen Low-Level- und High-Level-Metriken noch präziser zu modellieren. Eine interdisziplinäre Herangehensweise, die Erkenntnisse aus Informatik, Psychologie und Design kombiniert, könnte ebenfalls dazu beitragen, neue Erkenntnisse über die kognitive Wahrnehmung und Nutzungsmuster von Webseiten zu gewinnen. Dies könnte langfristig dazu beitragen, UX-Optimierungen nicht nur anhand technischer Kennzahlen vorzunehmen, sondern stärker auf die tatsächlichen Bedürfnisse der Nutzer auszurichten.

Zusammenfassend lässt sich feststellen, dass die in dieser Arbeit durchgeführte Untersuchung wertvolle Erkenntnisse über die Beziehung zwischen strukturellen Webseitenmerkmalen und UX- sowie Performance-Metriken liefert. Gleichzeitig zeigen sich jedoch eine Reihe von Limitationen, die zukünftige Forschung erforderlich machen. Eine breitere Datengrundlage, die Einbeziehung zusätzlicher Metriken und eine branchenübergreifende Analyse könnten dazu beitragen, die Aussagekraft der Ergebnisse weiter zu verbessern. Gleichzeitig bleibt die Herausforderung bestehen, technologische Entwicklungen und sich wandelnde Webstandards in zukünftige Untersuchungen einzubeziehen, um die langfristige Relevanz der gewonnenen Erkenntnisse zu gewährleisten. Während diese Arbeit eine erste Annäherung an das Thema bietet, bedarf es weiterer Forschung, um eine noch präzisere und verlässlichere Methode zur Vorhersage von High-Level-Metriken durch Low-Level-Metriken zu entwickeln. Es wird deutlich, dass der Bereich der automatisierten UX-Bewertung erhebliches Potenzial bietet, jedoch kontinuierlich an die technischen und konzeptionellen Entwicklungen des Webs angepasst werden muss. Damit schließt sich die Diskussion und überleitet in das abschließende Fazit, welches die zentralen Ergebnisse dieser Arbeit zusammenfasst und deren Bedeutung für die Forschung und Praxis reflektiert.